Polarization is a fundamental property of light that describes the geometric orientation of its oscillations. Capturing and analyzing this information provides a unique channel for scene understanding, with significant applications in fields such as reflection removal, 3D reconstruction, and transparent object detection. Recently, the advent of the Color-Polarization Filter Array (CPFA) sensor has revolutionized this domain. By integrating a 2×2 Polarization Filter Array (PFA) with a 4×4 Quad-Bayer Color Filter Array (CFA) using Division-of-Focal-Plane (DoFP) technology, CPFA sensors can capture a scene's 12-channel color-polarization information (RGB at 0°, 45°, 90°, and 135°) in a single snapshot. However, this capability comes at a cost: the raw sensor output is a sparse mosaic where each pixel records only one of the 12 possible channels. Consequently, a high-fidelity demosaicing process is an essential prerequisite for any downstream task.



The CPFA demosaicing problem is substantially more challenging than traditional Bayer demosaicing due to the greater data sparsity and the complex, interdependent correlations between color and polarization channels. While existing methods can be broadly classified into interpolation-based, model-based, and deep learning-based approaches, a dominant pipeline has emerged, particularly influencing the deep learning paradigm. This pipeline first subsamples the raw mosaic by polarization angle to create four half-resolution, Bayer-patterned images. Standard Bayer demosaicing is then applied to each, yielding a 12-channel, half-resolution tensor that is subsequently processed for final reconstruction.



This established pipeline, however, is built upon a fundamentally flawed premise. Bayer demosaicing algorithms operate under the assumption that the pixels being interpolated are spatially adjacent in the original scene. The subsampling process doesn’t comply with this assumption, as the pixels in the generated Bayer mosaics are sourced from non-adjacent positions on the sensor. It also forces the native Quad-Bayer sampling structure into a classic Bayer interpolation framework. This will introduce non-local color artifacts and false gradients. When these initial errors propagate through subsequent processing stages, either advanced interpolation or a deep neural network, they become deeply entangled, forcing the model to solve a convoluted artifact correction problem, which significantly harms demosaicing efficiency and limits final accuracy.



To overcome these limitations, we argue for a new pipeline that reframes the task as a well-defined, full-resolution image restoration problem. Rather than attempting to force CPFA data into a Bayer-like structure, we propose treating the raw data as 12 highly sparse, full-resolution channels. The primary goal is to perform an initial densification that produces predictable, spatially-local artifacts without introducing fictitious information. To this end, we employ **nearest-neighbor interpolation** based on Voronoi tessellation. While more sophisticated **scattered data interpolation** methods exist, they often introduce artificial smoothness, destroying high-frequency details and creating complex artifacts that are difficult for a neural network to unlearn. In contrast, nearest-neighbor interpolation produces simple, block-like artifacts that are totally predictable, presenting the network with a clean and structured artifact removal task.



With a well-posed input, the final challenge lies in designing a network architecture that respects the physical properties of the 12-channel data. Most deep learning methods conflate all 12 channels at the input layer, forcing the network to implicitly disentangle the deeply fused features. We propose a more structured approach by analyzing the features to be reconstructed. We identify two distinct types: **intra-polarization features** (e.g., semantic content, edges, textures), which are similar across the four polarization angles, and **inter-polarization features**, which capture the variance in reflected light intensity and model the physical principles of polarization. This insight leads to our proposed split-stream and fusion framework. We process the data in four parallel streams, grouped by polarization angle (RGB@0°, RGB@45°, etc.). Shared network modules are used to efficiently reconstruct the common intra-polarization features, while a dedicated fusion mechanism is designed to explicitly model the complex inter-polarization correlations. This architecture enables the network to learn both the semantic content of the scene and the underlying physics of light reflection in a more direct and effective manner.



